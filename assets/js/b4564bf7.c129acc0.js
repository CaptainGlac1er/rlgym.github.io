"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4334],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>y});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=r.createContext({}),p=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},c=function(e){var n=p(e.components);return r.createElement(s.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},g=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(t),g=a,y=m["".concat(s,".").concat(g)]||m[g]||u[g]||i;return t?r.createElement(y,o(o({ref:n},c),{},{components:t})):r.createElement(y,o({ref:n},c))}));function y(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=g;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[m]="string"==typeof e?e:a,o[1]=l;for(var p=2;p<i;p++)o[p]=t[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}g.displayName="MDXCreateElement"},1370:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var r=t(7462),a=(t(7294),t(3905));const i={title:"SB3 with a Single Game Instance"},o=void 0,l={unversionedId:"tools/sb3_single_env_wrapper",id:"tools/sb3_single_env_wrapper",title:"SB3 with a Single Game Instance",description:"Using SB3 with a Single Game Instance",source:"@site/docs/tools/sb3_single_env_wrapper.md",sourceDirName:"tools",slug:"/tools/sb3_single_env_wrapper",permalink:"/rlgym.github.io/docs/tools/sb3_single_env_wrapper",draft:!1,tags:[],version:"current",frontMatter:{title:"SB3 with a Single Game Instance"},sidebar:"tutorialSidebar",previous:{title:"SB3 with Multiple Game Instances",permalink:"/rlgym.github.io/docs/tools/sb3_multi_env_wrapper"},next:{title:"FAQs",permalink:"/rlgym.github.io/docs/faq/"}},s={},p=[{value:"Using SB3 with a Single Game Instance",id:"using-sb3-with-a-single-game-instance",level:2}],c={toc:p};function m(e){let{components:n,...t}=e;return(0,a.kt)("wrapper",(0,r.Z)({},c,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"using-sb3-with-a-single-game-instance"},"Using SB3 with a Single Game Instance"),(0,a.kt)("p",null,"Unfortunately, SB3 does not natively support the concept of self-play. However, ",(0,a.kt)("inlineCode",{parentName:"p"},"rlgym-tools")," provides a wrapper to get around this by treating each agent in a single match as though it were its own environment within SB3. With this, we can train more than one copy of our agent in one instance of the game through self-play. Let's see how we can set up an RLGym environment, wrap it in the ",(0,a.kt)("inlineCode",{parentName:"p"},"SB3SingleInstanceEnv"),",  and use the SB3 implementation of PPO to train it with self-play for 1,000,000 timesteps."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# import the gym and stable baselines 3 libraries\nimport rlgym\nfrom stable_baselines3.ppo import PPO\nfrom rlgym_tools.sb3_utils import SB3SingleInstanceEnv\n\n# setup the RLGym environment\ngym_env = rlgym.make(use_injector=True, self_play=True)\n\n# wrap the RLGym environment with the single instance wrapper\nenv = SB3SingleInstanceEnv(gym_env)\n\n# create a PPO instance and start learning\nlearner = PPO(policy="MlpPolicy", env=env, verbose=1)\nlearner.learn(1_000_000)\n')))}m.isMDXComponent=!0}}]);