"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5663],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>d});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},m=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},g=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),c=p(n),g=a,d=c["".concat(s,".").concat(g)]||c[g]||u[g]||i;return n?r.createElement(d,o(o({ref:t},m),{},{components:n})):r.createElement(d,o({ref:t},m))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=g;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[c]="string"==typeof e?e:a,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}g.displayName="MDXCreateElement"},4244:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var r=n(7462),a=(n(7294),n(3905));const i={sidebar_position:2},o="Getting Started",l={unversionedId:"getting-started/getting-started",id:"getting-started/getting-started",title:"Getting Started",description:"Setting up an Environment",source:"@site/docs/getting-started/getting-started.md",sourceDirName:"getting-started",slug:"/getting-started/",permalink:"/rlgym.github.io/docs/getting-started/",draft:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/rlgym.github.io/docs/introduction"},next:{title:"Documentation",permalink:"/rlgym.github.io/docs/category/documentation"}},s={},p=[{value:"Setting up an Environment",id:"setting-up-an-environment",level:2},{value:"Interacting With the Game",id:"interacting-with-the-game",level:2},{value:"Training a Simple Agent with PPO",id:"training-a-simple-agent-with-ppo",level:2}],m={toc:p};function c(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"getting-started"},"Getting Started"),(0,a.kt)("h2",{id:"setting-up-an-environment"},"Setting up an Environment"),(0,a.kt)("p",null,"Once RLGym is ",(0,a.kt)("a",{parentName:"p",href:"https://rlgym.github.io/docs-page.html#installation"},"installed"),", simply import the library and call the ",(0,a.kt)("inlineCode",{parentName:"p"},"make()")," function."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import rlgym\n\n\nenv = rlgym.make()\n")),(0,a.kt)("p",null,"This will configure Rocket League with the default parameters that come with RLGym."),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"make")," function comes with a number of optional parameters, which are explained in the ",(0,a.kt)("a",{parentName:"p",href:"https://rlgym.github.io/docs-page.html#documentation"},"Documentation")," section of the wiki.\nFor convenience, all the ",(0,a.kt)("inlineCode",{parentName:"p"},"make")," parameters and their types are listed here:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"game_speed: int = 100\ntick_skip: int = 8\nspawn_opponents: bool = True\nself_play: bool = False\nrandom_resets: bool = False\nteam_size: int = 1\nterminal_conditions: List[object] = (TimeoutCondition(225), GoalScoredCondition())\nreward_fn: object = DefaultReward()\nobs_builder: object = DefaultObs()\naction_parser: object = DefaultAction()\nstate_setter: object = DefaultState()\nlaunch_preference: str = LaunchPreference.EPIC\npath_to_rl: str = None,\nuse_injector: bool = False\nforce_paging: bool = False\n")),(0,a.kt)("h2",{id:"interacting-with-the-game"},"Interacting With the Game"),(0,a.kt)("p",null,"To interact with the game, simply treat it like any other OpenAI Gym environment:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import rlgym\n\nenv = rlgym.make()\n\nwhile True:\n    obs = env.reset()\n    done = False\n\n    while not done:\n      #Here we sample a random action. If you have an agent, you would get an action from it here.\n      action = env.action_space.sample()\n\n      next_obs, reward, done, gameinfo = env.step(action)\n\n      obs = next_obs\n")),(0,a.kt)("h2",{id:"training-a-simple-agent-with-ppo"},"Training a Simple Agent with PPO"),(0,a.kt)("p",null,"You can now train an agent with your learning algorithm of choice! Because RLGym follows the OpenAI Gym API, any of the common Reinforcement Learning libraries should be supported.\nThe following is an example of how to train an agent in the default RLGym environment using an implementation of PPO from the ",(0,a.kt)("a",{parentName:"p",href:"https://stable-baselines3.readthedocs.io/en/master/"},"Stable Baselines 3")," library."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import rlgym\nfrom stable_baselines3 import PPO\n\n#Make the default rlgym environment\nenv = rlgym.make()\n\n#Initialize PPO from stable_baselines3\nmodel = PPO("MlpPolicy", env=env, verbose=1)\n\n#Train our agent!\nmodel.learn(total_timesteps=int(1e6))\n')),(0,a.kt)("p",null,"And just like that a Rocket League agent is being trained!"),(0,a.kt)("p",null,"The default configuration of RLGym will not produce a competent game-playing agent.\nThis configuration is meant as a testing ground for users to quickly verify that they have installed RLGym successfully, and their learning algorithm is working.\nWhen the default reward is maximized, the agent should have zero angular velocity at all times."),(0,a.kt)("p",null,"To train a game-playing agent, users will need to configure an RLGym environment with an appropriate ",(0,a.kt)("a",{parentName:"p",href:"https://rlgym.github.io/docs-page.html#reward-functions"},"Reward Function"),", ",(0,a.kt)("a",{parentName:"p",href:"https://rlgym.github.io/docs-page.html#observation-builders"},"Observation Builder"),", and a set of ",(0,a.kt)("a",{parentName:"p",href:"https://rlgym.github.io/docs-page.html#terminal-conditions"},"Terminal Conditions"),".\nTo learn about configuring a custom environment, read through our ",(0,a.kt)("a",{parentName:"p",href:"https://rlgym.github.io/docs-page.html#tutorials"},"Tutorials"),"."),(0,a.kt)("p",null,"If you have any questions, comments, or just want to share your cool projects with us, we'd love to hear from you over on our ",(0,a.kt)("a",{parentName:"p",href:"https://discord.gg/NjAHcP32Ae"},"community discord"),"!"))}c.isMDXComponent=!0}}]);