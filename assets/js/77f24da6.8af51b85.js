"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9874],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>h});var a=t(7294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),c=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},u=function(e){var n=c(e.components);return a.createElement(l.Provider,{value:n},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},f=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=c(t),f=r,h=p["".concat(l,".").concat(f)]||p[f]||m[f]||o;return t?a.createElement(h,i(i({ref:n},u),{},{components:t})):a.createElement(h,i({ref:n},u))}));function h(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=f;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[p]="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=t[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}f.displayName="MDXCreateElement"},5861:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var a=t(7462),r=(t(7294),t(3905));const o={},i="Using stable-baselines3",s={unversionedId:"sb3",id:"sb3",title:"Using stable-baselines3",description:"Single Game Instance",source:"@site/tools/sb3.md",sourceDirName:".",slug:"/sb3",permalink:"/tools/sb3",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Saving and Loading",permalink:"/tools/saving_and_loading"}},l={},c=[{value:"Single Game Instance",id:"single-game-instance",level:2},{value:"Multiple Game Instances",id:"multiple-game-instances",level:2}],u={toc:c},p="wrapper";function m(e){let{components:n,...t}=e;return(0,r.kt)(p,(0,a.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"using-stable-baselines3"},"Using stable-baselines3"),(0,r.kt)("h2",{id:"single-game-instance"},"Single Game Instance"),(0,r.kt)("p",null,"Unfortunately, SB3 does not natively support the concept of self-play.\nHowever, ",(0,r.kt)("inlineCode",{parentName:"p"},"rlgym-tools")," provides a wrapper to get around this by treating each agent in a single match as though it were its own environment within SB3.\nWith this, we can train more than one copy of our agent in one instance of the game through self-play.\nLet's see how we can set up an RLGym environment, wrap it in the ",(0,r.kt)("inlineCode",{parentName:"p"},"SB3SingleInstanceEnv"),", and use the SB3 implementation of PPO to train it with self-play for 1,000,000 timesteps."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# import the gym and stable baselines 3 libraries\nimport rlgym\nfrom stable_baselines3.ppo import PPO\nfrom rlgym_tools.sb3_utils import SB3SingleInstanceEnv\n\n# setup the RLGym environment\ngym_env = rlgym.make(use_injector=True, self_play=True)\n\n# wrap the RLGym environment with the single instance wrapper\nenv = SB3SingleInstanceEnv(gym_env)\n\n# create a PPO instance and start learning\nlearner = PPO(policy="MlpPolicy", env=env, verbose=1)\nlearner.learn(1_000_000)\n')),(0,r.kt)("h2",{id:"multiple-game-instances"},"Multiple Game Instances"),(0,r.kt)("p",null,"Most consumer hardware is capable of running more than just one instance of Rocket League at a time.\nWe would like to take advantage of that when training an agent for complex tasks like playing the game effectively.\nFortunately, SB3 supports multi-processing natively, so we can use the ",(0,r.kt)("inlineCode",{parentName:"p"},"rlgym-tools")," multi-instance wrapper to launch as many parallel game instances as we like."),(0,r.kt)("p",null,"Using this wrapper requires us to make a special function that will construct a ",(0,r.kt)("inlineCode",{parentName:"p"},"rlgym.envs.Match")," object with all the necessary settings and objects.\nThis is because RLGym needs each instance of the environment to exist in an independent process, so our wrapper will use this function to create each environment after its process has been spawned.\nLet's look at an example for how to use the ",(0,r.kt)("inlineCode",{parentName:"p"},"SB3MultipleInstanceEnv")," with the SB3 implementation of  PPO to train an agent for 1,000,000 timesteps with 2 instances of the game open."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# Here we import the Match object and our multi-instance wrapper\nfrom rlgym.envs import Match\nfrom rlgym_tools.sb3_utils import SB3MultipleInstanceEnv\n\n# Since we can\'t use the normal rlgym.make() function.\n# We need to import all the default configuration objects to give to our Match.\nfrom rlgym.utils.reward_functions import DefaultReward\nfrom rlgym.utils.obs_builders import DefaultObs\nfrom rlgym.utils.state_setters import DefaultState\nfrom rlgym.utils.terminal_conditions.common_conditions import TimeoutCondition\n\n# Finally, we import the SB3 implementation of PPO.\nfrom stable_baselines3.ppo import PPO\n\n"""\n    This is the function we need to provide to our SB3MultipleInstanceEnv to construct a match.\n    Note that this function MUST return a Match object.\n"""\ndef get_match():\n    """\n        Here we configure our Match.\n        If you want to use custom configuration objects.\n        Make sure to replace the default arguments here with instances of the objects you want.\n    """\n    return Match(\n        reward_function=DefaultReward(),\n        terminal_conditions=[TimeoutCondition(225)],\n        obs_builder=DefaultObs(),\n        state_setter=DefaultState(),\n\n        self_play=True,\n    )\n\n\n#If we want to spawn new processes, we have to make sure our program starts in a proper Python entry point.\nif __name__ == "__main__":\n    """\n        Now all we have to do is make an instance of the SB3MultipleInstanceEnv and pass it our get_match function.\n        The number of instances we\'d like to open, and how long it should wait between instances.\n        This wait_time argument is important because if multiple Rocket League clients are opened in quick succession.\n        They will cause each other to crash.\n        The exact reason this happens is unknown to us,\n        but the easiest solution is to delay for some period of time between launching clients.\n        The amount of required delay will depend on your hardware.\n        So make sure to change this number if your Rocket League clients are crashing before they fully launch.\n    """\n    env = SB3MultipleInstanceEnv(match_func_or_matches=get_match, num_instances=2, wait_time=20)\n    learner = PPO(policy="MlpPolicy", env=env, verbose=1)\n    learner.learn(1_000_000)\n')))}m.isMDXComponent=!0}}]);