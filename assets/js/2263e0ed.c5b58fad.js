"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[476],{3905:(e,r,t)=>{t.d(r,{Zo:()=>p,kt:()=>w});var n=t(7294);function a(e,r,t){return r in e?Object.defineProperty(e,r,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[r]=t,e}function o(e,r){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);r&&(n=n.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var r=1;r<arguments.length;r++){var t=null!=arguments[r]?arguments[r]:{};r%2?o(Object(t),!0).forEach((function(r){a(e,r,t[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(t,r))}))}return e}function l(e,r){if(null==e)return{};var t,n,a=function(e,r){if(null==e)return{};var t,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],r.indexOf(t)>=0||(a[t]=e[t]);return a}(e,r);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],r.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var d=n.createContext({}),s=function(e){var r=n.useContext(d),t=r;return e&&(t="function"==typeof e?e(r):i(i({},r),e)),t},p=function(e){var r=s(e.components);return n.createElement(d.Provider,{value:r},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var r=e.children;return n.createElement(n.Fragment,{},r)}},u=n.forwardRef((function(e,r){var t=e.components,a=e.mdxType,o=e.originalType,d=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),c=s(t),u=a,w=c["".concat(d,".").concat(u)]||c[u]||m[u]||o;return t?n.createElement(w,i(i({ref:r},p),{},{components:t})):n.createElement(w,i({ref:r},p))}));function w(e,r){var t=arguments,a=r&&r.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=u;var l={};for(var d in r)hasOwnProperty.call(r,d)&&(l[d]=r[d]);l.originalType=e,l[c]="string"==typeof e?e:a,i[1]=l;for(var s=2;s<o;s++)i[s]=t[s];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}u.displayName="MDXCreateElement"},6269:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>d,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>s});var n=t(7462),a=(t(7294),t(3905));const o={},i=void 0,l={unversionedId:"tools/reward_functions",id:"tools/reward_functions",title:"reward_functions",description:"Extra Rewards",source:"@site/docs/tools/reward_functions.md",sourceDirName:"tools",slug:"/tools/reward_functions",permalink:"/rlgym.github.io/docs/tools/reward_functions",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tools/reward_functions.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"observation_builders",permalink:"/rlgym.github.io/docs/tools/observation_builders"},next:{title:"saving_and_loading",permalink:"/rlgym.github.io/docs/tools/saving_and_loading"}},d={},s=[{value:"Extra Rewards",id:"extra-rewards",level:2},{value:"AnnealRewards",id:"annealrewards",level:3},{value:"DiffReward",id:"diffreward",level:3},{value:"DistributeRewards",id:"distributerewards",level:3},{value:"MultiplyRewards",id:"multiplyrewards",level:3}],p={toc:s};function c(e){let{components:r,...t}=e;return(0,a.kt)("wrapper",(0,n.Z)({},p,t,{components:r,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"extra-rewards"},"Extra Rewards"),(0,a.kt)("p",null,(0,a.kt)("inlineCode",{parentName:"p"},"rlgym-tools")," comes with a number of potentially useful reward functions which are not packaged directly with RLGym. Here we will provide a brief summary of each reward function and how to use it."),(0,a.kt)("h3",{id:"annealrewards"},"AnnealRewards"),(0,a.kt)("p",null,"If we're interested in training an agent to accomplish some task by mastering increasingly complex reward functions (as in ",(0,a.kt)("a",{parentName:"p",href:"https://jmlr.org/papers/volume21/20-212/20-212.pdf"},"curriculum learning"),"), we may want to smoothly transition from one reward function to the next. AnnealRewards does this by weighting the current reward stage with the next one as ",(0,a.kt)("inlineCode",{parentName:"p"},"y*current_reward + (1 - y)*current_reward")," where ",(0,a.kt)("inlineCode",{parentName:"p"},"y = counter / max_count"),". The counter used here can be specified through one of three modes: ",(0,a.kt)("inlineCode",{parentName:"p"},"STEP"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"TOUCH"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"GOAL"),". At each call to ",(0,a.kt)("inlineCode",{parentName:"p"},"AnnealRewards.get_reward()"),", the counter is incremented based on the mode provided. The ",(0,a.kt)("inlineCode",{parentName:"p"},"STEP")," mode will cause the counter to be incremented by 1 at every call to ",(0,a.kt)("inlineCode",{parentName:"p"},"get_reward"),", the ",(0,a.kt)("inlineCode",{parentName:"p"},"TOUCH")," mode will increment the counter every time the player touches the ball, and the ",(0,a.kt)("inlineCode",{parentName:"p"},"GOAL")," mode will increment the counter every time the player scores a goal."),(0,a.kt)("p",null,"To specify the max number of steps that each reward function should be present for, just pass the value as an ",(0,a.kt)("inlineCode",{parentName:"p"},"int")," to the ",(0,a.kt)("inlineCode",{parentName:"p"},"AnnealRewards")," constructor in a list with the ",(0,a.kt)("inlineCode",{parentName:"p"},"RewardFunction")," objects. Let's look at an example where we set up an ",(0,a.kt)("inlineCode",{parentName:"p"},"AnnealRewards")," object to  smoothly transition the reward function from ",(0,a.kt)("inlineCode",{parentName:"p"},"VelocityReward")," to ",(0,a.kt)("inlineCode",{parentName:"p"},"VelocityPlayerToBallReward"),"."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import rlgym\nfrom rlgym_tools.extra_rewards import AnnealRewards\nfrom rlgym.utils.reward_functions.common_rewards import VelocityPlayerToBallReward, TouchBallReward\n\ndef anneal_rewards_fn():\n    \n    # These are arbitrary values that may not result in a good model.\n    max_steps = 100_000\n    reward1 = VelocityPlayerToBallReward()\n    reward2 = TouchBallReward(aerial_weight=0.2)\n\n    alternating_rewards_steps = [reward1, max_steps, reward2]\n\n    return AnnealRewards(*alternating_rewards_steps, mode=AnnealRewards.STEP)\n\nenv = rlgym.make(reward_fn=anneal_rewards_fn())\n")),(0,a.kt)("h3",{id:"diffreward"},"DiffReward"),(0,a.kt)("p",null,"This simple function will return the difference in reward between time steps (e.g. reward_at_t1 - reward_at_t0). This may be useful if we're interested in rewarding the agent for its acceleration, rather than velocity."),(0,a.kt)("p",null,"Example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import rlgym\nfrom rlgym_tools.extra_rewards import DiffReward\nfrom rlgym.utils.reward_functions.common_rewards import VelocityReward\n\nvel_reward = VelocityReward()\naccel_reward = DiffReward(vel_reward)\n\nenv = rlgym.make(reward_fn=accel_reward)\n")),(0,a.kt)("h3",{id:"distributerewards"},"DistributeRewards"),(0,a.kt)("p",null,"This reward is similar to the method used by OpenAI Five to distribute the credit of a reward during a team game across the agents in the match. Read Appendix G from ",(0,a.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/1912.06680.pdf"},"their paper")," to learn more."),(0,a.kt)("p",null,"Example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import rlgym\nfrom rlgym_tools.extra_rewards import DistributeRewards\nfrom rlgym.utils.reward_functions.common_rewards import EventReward\n\ngoal_reward = EventReward(goal=1, concede=-1)\ndistrib_reward = DistributeRewards(goal_reward, team_spirit=0.3)\n\nenv = rlgym.make(reward_fn=distrib_reward)\n")),(0,a.kt)("h3",{id:"multiplyrewards"},"MultiplyRewards"),(0,a.kt)("p",null,"As the name implies, this will take any number of reward functions and return their product at each timestep."),(0,a.kt)("p",null,"Example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"import rlgym\nfrom rlgym_tools.extra_rewards import MultiplyRewards\nfrom rlgym.utils.reward_functions.common_rewards import EventReward, VelocityPlayerToBallReward\n\ntouch_reward = EventReward(touch=1)\nvelocity_to_ball_reward = VelocityPlayerToBallReward()\n\nrewards = [touch_reward, velocity_to_ball_reward]\nmultiply_reward = MultiplyRewards(rewards)\n\nenv = rlgym.make(reward_fn=multiply_reward)\n")))}c.isMDXComponent=!0}}]);