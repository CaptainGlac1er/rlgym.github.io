"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6820],{3905:(e,r,t)=>{t.d(r,{Zo:()=>d,kt:()=>f});var a=t(7294);function n(e,r,t){return r in e?Object.defineProperty(e,r,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[r]=t,e}function o(e,r){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);r&&(a=a.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var r=1;r<arguments.length;r++){var t=null!=arguments[r]?arguments[r]:{};r%2?o(Object(t),!0).forEach((function(r){n(e,r,t[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(t,r))}))}return e}function i(e,r){if(null==e)return{};var t,a,n=function(e,r){if(null==e)return{};var t,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],r.indexOf(t)>=0||(n[t]=e[t]);return n}(e,r);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],r.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var s=a.createContext({}),c=function(e){var r=a.useContext(s),t=r;return e&&(t="function"==typeof e?e(r):l(l({},r),e)),t},d=function(e){var r=c(e.components);return a.createElement(s.Provider,{value:r},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var r=e.children;return a.createElement(a.Fragment,{},r)}},p=a.forwardRef((function(e,r){var t=e.components,n=e.mdxType,o=e.originalType,s=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),m=c(t),p=n,f=m["".concat(s,".").concat(p)]||m[p]||u[p]||o;return t?a.createElement(f,l(l({ref:r},d),{},{components:t})):a.createElement(f,l({ref:r},d))}));function f(e,r){var t=arguments,n=r&&r.mdxType;if("string"==typeof e||n){var o=t.length,l=new Array(o);l[0]=p;var i={};for(var s in r)hasOwnProperty.call(r,s)&&(i[s]=r[s]);i.originalType=e,i[m]="string"==typeof e?e:n,l[1]=i;for(var c=2;c<o;c++)l[c]=t[c];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}p.displayName="MDXCreateElement"},8913:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>s,contentTitle:()=>l,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var a=t(7462),n=(t(7294),t(3905));const o={},l="Misc Rewards",i={unversionedId:"documentation/reward_functions/common_rewards/misc_rewards",id:"documentation/reward_functions/common_rewards/misc_rewards",title:"Misc Rewards",description:"Misc rewards.",source:"@site/docs/documentation/reward_functions/common_rewards/misc_rewards.md",sourceDirName:"documentation/reward_functions/common_rewards",slug:"/documentation/reward_functions/common_rewards/misc_rewards",permalink:"/rlgym.github.io/docs/documentation/reward_functions/common_rewards/misc_rewards",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Conditional rewards",permalink:"/rlgym.github.io/docs/documentation/reward_functions/common_rewards/conditional_rewards"},next:{title:"Player Ball Rewards",permalink:"/rlgym.github.io/docs/documentation/reward_functions/common_rewards/player_ball_rewards"}},s={},c=[{value:"Velocity Reward",id:"velocity-reward",level:2},{value:"Save Boost Reward",id:"save-boost-reward",level:2},{value:"Constant Reward",id:"constant-reward",level:2},{value:"Align Ball To Goal",id:"align-ball-to-goal",level:2}],d={toc:c};function m(e){let{components:r,...t}=e;return(0,n.kt)("wrapper",(0,a.Z)({},d,t,{components:r,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"misc-rewards"},"Misc Rewards"),(0,n.kt)("p",null,"Misc rewards."),(0,n.kt)("h2",{id:"velocity-reward"},(0,n.kt)("a",{parentName:"h2",href:"https://github.com/lucas-emery/rocket-league-gym/blob/7f07bfa980b84eea11627939dd7d7b1689efcfa7/rlgym/utils/reward_functions/common_rewards/misc_rewards.py#L56"},"Velocity Reward")),(0,n.kt)("p",null,"Velocity reward is a simple function to make sure models can be trained.\nThe velocity reward function returns either the positive or negative magnitude of the agent's velocity, determined by the ",(0,n.kt)("inlineCode",{parentName:"p"},"negative")," flag in the constructor."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"VelocityReward(negative=False)\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from rlgym.utils.reward_functions.misc_rewards import VelocityReward\n\nvelocity_reward = VelocityReward()\n")),(0,n.kt)("h2",{id:"save-boost-reward"},(0,n.kt)("a",{parentName:"h2",href:"https://github.com/lucas-emery/rocket-league-gym/blob/7f07bfa980b84eea11627939dd7d7b1689efcfa7/rlgym/utils/reward_functions/common_rewards/misc_rewards.py#L69"},"Save Boost Reward")),(0,n.kt)("p",null,"Each step the agent is rewarded with ",(0,n.kt)("inlineCode",{parentName:"p"},"sqrt(player.boost_amount)"),".\nWe take the square root here because, intuitively, the difference between 0 and 20 boost is more impactful on the game than the difference between 80 and 100 boost."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"SaveBoostReward()\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from rlgym.utils.reward_functions.misc_rewards import SaveBoostReward\n\nsave_boost_reward = SaveBoostReward()\n")),(0,n.kt)("h2",{id:"constant-reward"},(0,n.kt)("a",{parentName:"h2",href:"https://github.com/lucas-emery/rocket-league-gym/blob/7f07bfa980b84eea11627939dd7d7b1689efcfa7/rlgym/utils/reward_functions/common_rewards/misc_rewards.py#L78"},"Constant Reward")),(0,n.kt)("p",null,"Provides a constant reward of 1 to agent every step."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"ConstantReward()\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from rlgym.utils.reward_functions.misc_rewards import ConstantReward\n\nconstant_reward = ConstantReward()\n")),(0,n.kt)("h2",{id:"align-ball-to-goal"},(0,n.kt)("a",{parentName:"h2",href:"https://github.com/lucas-emery/rocket-league-gym/blob/7f07bfa980b84eea11627939dd7d7b1689efcfa7/rlgym/utils/reward_functions/common_rewards/misc_rewards.py#L86"},"Align Ball To Goal")),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"Determine which team the agent is on (and by extension which net we should be attacking / defending)"),(0,n.kt)("li",{parentName:"ol"},"Compute defensive reward for when the agent aligns the ball away from their goal"),(0,n.kt)("li",{parentName:"ol"},"Computer offensive reward for when the agent aligns the ball towards the opponents goal"),(0,n.kt)("li",{parentName:"ol"},"Sum defensive and offensive rewards and return the total")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"AlignBallGoal(defense=1., offense=1.)\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"from rlgym.utils.reward_functions.misc_rewards import AlignBallGoal\n\nalign_ball_to_goal = AlignBallGoal()\n")))}m.isMDXComponent=!0}}]);